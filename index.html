<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	Used for LHMP 2022
-->
<html>
	<head>
		<title>LHMP 2022 workshop</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img width="100px" src="images/lhmp-icon-2022.svg" alt="" /></span>
						<h1>4th Workshop on <br /> <strong>Long-term Human Motion Prediction</strong></h1>
						<p>2022 IEEE International Conference on Robotics and Automation (ICRA)</br>May 23, 2022 Philadelphia (PA), USA</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">About the workshop</a></li>
							<li><a href="#registration" class="active">Registration</a></li>
							<li><a href="#program" class="active">Program</a></li>
							<li><a href="#organizers">Organizers</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Anticipating human motion is a key skill for intelligent systems that share a space or interact with humans</h2>
										</header>
										<p>Accurate long-term
						predictions of human movement trajectories, body poses, actions or activities may significantly
						improve the ability of robots to <strong>plan ahead, anticipate the effects of their actions or
							to foresee hazardous situations</strong>. The topic has received increasing attention in
						recent years across several scientific communities with a growing spectrum of applications in
						service robots, self-driving cars, collaborative manipulators or tracking and surveillance.
						</br></br>
						This workshop is fourth in a series of ICRA 2019-2022 events. The aim of this workshop is to
						bring together researchers and practitioners from different communities and to discuss recent
						developments in this field, promising approaches, their limitations, benchmarking techniques and
						open challenges.</p>
										<ul class="actions">
											<li><a href="past-events.html" class="button">Learn More</a></li>
										</ul>
									</div>
									<span class="image"><img src="images/lhmp-spencer-2.png" alt="" /></span>
								</div>
							</section>

				<section id="registration" class="main style1">
			<div class="container">
				<header class="major">
					<h2>Registration</h2>
				</header>

					<p>Online participation in this workshop is free of charge. To join the virtual workshop, use this link: <a href="https://zoom.us/j/7746263634?pwd=UFVYelo5RGE2cTNWVmdMekx4TzZjUT09">https://zoom.us/j/7746263634?pwd=UFVYelo5RGE2cTNWVmdMekx4TzZjUT09</a>.
						
					For in-person participation, please follow to the <a href="https://www.icra2022.org/registration/registration-fees">ICRA conference and workshops registration page</a>.</p>

					
			</div>
		</section>

<!--
							<section id="first" class="main special">
								<header class="major">
									<h2>Magna veroeros</h2>
								</header>
								<ul class="features">
									<li>
										<span class="icon solid major style1 fa-code"></span>
										<h3>Ipsum consequat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style3 fa-copy"></span>
										<h3>Amed sed feugiat</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
									<li>
										<span class="icon major style5 fa-gem"></span>
										<h3>Dolor nullam</h3>
										<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
									</li>
								</ul>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>


							<section id="second" class="main special">
								<header class="major">
									<h2>Ipsum consequat</h2>
									<p>Donec imperdiet consequat consequat. Suspendisse feugiat congue<br />
									posuere. Nulla massa urna, fermentum eget quam aliquet.</p>
								</header>
								<ul class="statistics">
									<li class="style1">
										<span class="icon solid fa-code-branch"></span>
										<strong>5,120</strong> Etiam
									</li>
									<li class="style2">
										<span class="icon fa-folder-open"></span>
										<strong>8,192</strong> Magna
									</li>
									<li class="style3">
										<span class="icon solid fa-signal"></span>
										<strong>2,048</strong> Tempus
									</li>
									<li class="style4">
										<span class="icon solid fa-laptop"></span>
										<strong>4,096</strong> Aliquam
									</li>
									<li class="style5">
										<span class="icon fa-gem"></span>
										<strong>1,024</strong> Nullam
									</li>
								</ul>
								<p class="content">Nam elementum nisl et mi a commodo porttitor. Morbi sit amet nisl eu arcu faucibus hendrerit vel a risus. Nam a orci mi, elementum ac arcu sit amet, fermentum pellentesque et purus. Integer maximus varius lorem, sed convallis diam accumsan sed. Etiam porttitor placerat sapien, sed eleifend a enim pulvinar faucibus semper quis ut arcu. Ut non nisl a mollis est efficitur vestibulum. Integer eget purus nec nulla mattis et accumsan ut magna libero. Morbi auctor iaculis porttitor. Sed ut magna ac risus et hendrerit scelerisque. Praesent eleifend lacus in lectus aliquam porta. Cras eu ornare dui curabitur lacinia.</p>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>

 -->



<section id="program" class="main style1">
			<div class="container">
				<header class="major">
					<h2>Program</h2>
				</header>

					<p>The program of this workshop includes 9 talks in several sessions, and a tutorial session on prediction methods and benchmarking.</p>

					<div class="table-wrapper">
						<table>
							<thead>
								<tr>
									<th>Time</th>
									<th>Speaker</th>
									<th>Topic</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>8:30 - 8:45 am EDT </br> 14:30 - 14:45 CEST</td>
									<td>Organizers</td>
									<td>Welcome and Introduction</td>
								</tr>
								<tr>
									<td></td>
									<td>Blue-sky session</td>
									<td></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>8:45 - 9:15 am EDT </br> 14:45 - 15:15 CEST</td>
									<td><strong><a href="http://www.iri.upc.edu/people/sanfeliu/">Alberto Sanfeliu</a></strong>, Universitat Politècnica de Catalunya</td>
									<td>Predicting human motion for human-robot interaction & collaboration</td>
									<td class='expanded-row-content hide-row'>Abstract: tbd</td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>9:15 - 9:45 am EDT </br> 15:15 - 15:45 CEST</td>
									<td><strong><a href="http://people.rennes.inria.fr/Julien.Pettre/">Julien Pettré</a></strong>, INRIA</td>
									<td>Predicting crowds: scales and data</td>
									<td class='expanded-row-content hide-row'>Abstract: Human trajectory prediction (HTP), which has mainly found applications in robotics, is now also finding points of convergence with the field of predictive crowd simulation (for traffic management in public environments for instance). This convergence accelerates in particular the ongoing transition of crowd simulators from knowledge-based models to data-driven ones. Through the exploration of this new field of applications, this presentation raises the question of the formulation of the prediction problem as we know it, but also of the nature of the data that are used for the modeling. In this presentation, I will give the perspective of the field of crowd simulation on these issues, and present some promising techniques for the acquisition of new data that are based on virtual reality. </td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>9:45 - 10:15 am EDT </br> 15:45 - 16:15 CEST</td>
									<td><strong><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html">Gerard Pons-Moll</a></strong>, University of Tübingen</td>
									<td>Virtual Humans — From appearance to behaviour</td>
									<td class='expanded-row-content hide-row'>Abstract: Modelling 3D virtual humans is necessary for VR/AR and to digitally transfer people to digital spaces — often referred to as the metaverse.  
									While there has been significant progress on modelling human appearance (how we look), modelling and capturing fine grained human behaviour in 3D has received much less attention.
									Accurately capturing human interactions with scenes and objects in 3D is challenging due to occlusions, complex poses, ambiguities in reconstructing objects in 3D, and limited recording volumes imposed by multi-view camera setups.
									In this talk, I will describe our recent works to capture and model how we behave and interact with the 3D world around us. I will present HPS, a method to capture and localize humans interacting in large 3D scenes from wearable sensors, as well as BEHAVE a recent dataset and method to capture 3D humans interacting with objects.  
									</td>
								</tr>
								<tr>
									<td>10:15 - 11:00 am EDT </br> 16:15 - 17:00 CEST</td>
									<td>Coffee break</td>
									<td></td>
								</tr>
								<tr>
									<td></td>
									<td>Body and Mind session</td>
									<td></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>11:00 - 11:30 am EDT </br> 17:00 - 17:30 CEST</td>
									<td><strong><a href="https://cos.northeastern.edu/people/dagmar-sternad/">Dagmar Sternad</a></strong>, Northeastern University</td>
									<td>Predicting actions and interactions: the human perspective</td>
									<td class='expanded-row-content hide-row'>Abstract: Anticipating motion of other agents and objects in the environment is key for successful behavior, both for robots and humans. The ability to predict is a core computational competence necessary for almost all aspects of human behavior, including social, cognitive, perceptual and action contexts. This talk will focus on the human perspective and present several lines of research examining predictive abilities and challenges in humans.

										</br></br>Our recent research investigated this fundamental ability in the context of sensorimotor interactions with a dynamic object: intercepting and catching a flying ball. To examine the developmental trajectory of predictive ability, we assessed participants between 5 and 92 years of age in a suite of custom-developed virtual games that provided millisecond-scale measures of actions in response to the flying ball. Results revealed age-related improvements in predictive motor behavior, with performance reaching adult levels by 12 years of age. This developmental progression provides a behavioral manifestation consistent with recent findings on cerebellar and cortical maturation. 

										</br></br>A second line of research scrutinized how humans continuous interact with dynamically complex objects, such as a cup of coffee. The internal dynamics in such objects creates complex nonlinear interaction forces that can be chaotic and essentially unpredictable for humans. We investigated how humans deal with such scenarios in a virtual environment where human participants transported a cup of coffee’, modeled by a cart and pendulum system. Results showed that humans learnt to simplify the interaction forces that made them more predictable.

										</br></br>A third line of research examined prediction within the motor control system in the context of postural control under perturbations. Catching a ball not only involves finely timed arm and hand movements with respect to the ball, but these rapid arm movements also create perturbing forces that destabilize postural balance. Maintaining upright posture requires anticipatory postural adjustments in trunk muscles that ensures the control of hand movements. We show such subtle adjustments in the context of catching a ball in both healthy and impaired populations.

										</br></br>These different lines of research demonstrate the pervasiveness of accurate prediction at all levels of successful behavior, and also show the challenges that also humans and not only robots face.
</td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>11:30 - 12:00 am EDT </br> 17:30 - 18:00 CEST</td>
									<td><strong><a href="http://wpage.unina.it/silrossi/index.html">Silvia Rossi</a></strong>, University of Naples</td>
									<td>Human and context awareness: Toward socially enhanced autonomous capabilities</td>
									<td class='expanded-row-content hide-row'>Abstract: To effectively exploit autonomous capabilities that are socially enhanced, a robot is required to sense its environment but also to understand what happens within it. Human awareness not only concerns the acknowledgment of the person’s position and pose within the environment but also the opportunity to understand the activity that the person is currently performing. For example, robotic personal assistance may be required to recognize the user’s Activities of Daily Living (eating, drinking, cooking, watching TV, using a mobile phone, etc.) or emergencies, such as fall detection. Moreover, the importance of interpreting and recognizing social and non-verbal signals during the interaction is generally well recognized within the social robotics community, but it plays a fundamental role also in service and collaborative robotics. For example, the interpretation of non-verbal cues, such as gaze, posture, and backchannels, can be used in the recognition of the person’s engagement during an interaction, the same could be used to evaluate the person’s discomfort or the disengagement from the current activity caused by the robot’s behavior in the shared environment. In this talk, we will discuss different approaches aiming at achieving socially enhanced autonomous robot behaviors from the interpretation of human behavior.</td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>12:00 - 12:30 pm EDT </br> 18:00 - 18:30 CEST</td>
									<td><strong><a href="https://web.mit.edu/krallen/www/">Kelsey Allen</a></strong>, DeepMind</td>
									<td>The surprising diversity of human tool use</td>
									<td class='expanded-row-content hide-row'>Abstract: People use tools every day – from forks and knives to computers and cellphones. Indeed, much of our interaction with the world is modulated by our tools. As a result, understanding how people use tools is critical to safely interacting with humans. In this talk, I will discuss our research into the vast and varied ways that people learn how to use new tools. I will highlight both humans’ rapid adaptivity, but also their surprising rigidity, in the face of new problems. I will also discuss how differences in lived experience, such as growing up with only one hand, can fundamentally alter the ways in which people approach physical problem-solving generally. For robot-human interaction, it will not be enough to model the motion of one ideal human. Instead, a diversity of humans is needed.</td>
								</tr>
								<tr>
									<td>12:30 - 1:45 pm EDT </br> 18:30 - 19:45 CEST</td>
									<td>Lunch break</td>
									<td></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>1:45 - 2:15 pm EDT </br> 19:45 - 20:15 CEST</td>
									<td><strong><a href="https://ece.illinois.edu/about/directory/faculty/krdc">Katherine Driggs-Campbell</a></strong>, University  of  Illinois</td>
									<td>Inference and prediction for safe interaction </td>
									<td class='expanded-row-content hide-row'>Abstract: Autonomous systems and robots are becoming prevalent in our everyday lives and changing the foundations of our way of life. However, the desirable impacts of autonomy are only achievable if the underlying algorithms can handle the unique challenges humans present. To design safe, trustworthy autonomy, we must transform how intelligent systems interact, influence, and predict human agents. In this talk, we'll discuss how inferring hidden states (e.g., driver traits, pedestrian intent, occluded agents) coupled with robust prediction methods can be used to improve decision-making and control in interactive settings. These methods are used to generate safe interactions between humans and mobile robots (sometimes with guarantees), which are demonstrated on fully equipped test vehicles and mobile robots.</td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>2:15 - 2:45 pm EDT </br> 20:15 - 20:45 CEST</td>
									<td><strong><a href="https://www.borisivanovic.com">Boris Ivanovic</a></strong>, Nvidia</td>
									<td>Effectively integrating prediction within the autonomous vehicle stack</td>
									<td class='expanded-row-content hide-row'>Abstract: tbd</td>
								</tr>
								<tr>
									<td></td>
									<td>Tutorial & Benchmarking session</td>
									<td></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>2:45 - 3:00 pm EDT </br> 20:45 - 21:00 CEST</td>
									<td><strong><a href="https://thedebugger811.github.io/">Parth Kothari</a></strong>, EPFL</td>
									<td>TrajNet++ update</td>
									<td class='expanded-row-content hide-row'>Link: <a href="https://www.aicrowd.com/challenges/trajnet-a-trajectory-forecasting-challenge">https://www.aicrowd.com/challenges/trajnet-a-trajectory-forecasting-challenge</a></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>3:00 - 3:15 pm EDT </br> 21:00 - 21:15 CEST</td>
									<td><strong><a href="https://rudenkoandrey.github.io/">Andrey Rudenko</a></strong>, Bosch</td>
									<td>The Atlas Benchmark presentation</td>
									<td class='expanded-row-content hide-row'>Link: tbd</td>
								</tr>
								<tr>
									<td>3:15 - 4:00 pm EDT </br> 21:15 - 22:00 CEST</td>
									<td>Coffee break</td>
									<td></td>
								</tr>
								<tr onClick='toggleRow(this)'>
									<td>4:00 - 4:30 pm EDT </br> 22:00 - 22:30 CEST</td>
									<td><strong><a href="https://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a>, <a href="http://www.cs.cmu.edu/~arunvenk/">Arun Venkatraman</a></strong>, Aurora</td>
									<td>Imitation learning and forecasting: It’s only a game!</td>
									<td class='expanded-row-content hide-row'>Abstract: A core challenge in self-driving is reasoning about how actors on the road interact to affect each other’s motions. Recent advances in machine learning have enabled powerful forecasting techniques that jointly reason about such interactions. However, many of these approaches treat forecasting in isolation from downstream decision making. This leads to a fundamental misalignment in objectives where better forecasts do not necessarily translate into better end-to-end behavior. 

</br></br>In this talk, we will explore a clean-sheet approach to forecasting and decision making that builds on a singular objective – imitate expert human driving. We will present a unified, game-theoretic framework for imitation learning. We will view forecasting as an adversary that discriminates between human and robot driving. Finally, we will show how this framework leads to forecasts that enable human-like, predictable, and interpretable behavior on the road.
</td>
								</tr>
								<tr>
									<td>4:30 - 5:00 pm EDT </br> 22:30 - 23:00 CEST</td>
									<td>Organizers</td>
									<td>Discussion and conclusions</td>
								</tr>
							</tbody>
						</table>
					</div>

					
			</div>
		</section>




		<section id="organizers" class="main style1">
			<div class="container">
				<header class="major special">
					<h2>Organizers</h2>
				</header>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/Andrey_Rudenko.jpg" alt="" /></span>
							<h5> <a href="https://rudenkoandrey.github.io"><strong>Andrey Rudenko</strong>,</br>
									Robert Bosch GmbH</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/luigi-palmieri-foto.1024x1024.jpg"
									alt="" /></span>
							<h5><a href="https://palmieri.github.io/"><strong>Luigi Palmieri</strong>,</br> Robert Bosch
									GmbH</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/Andrea-Bajcsy.jpg" alt="" /></span>
							<h5> <a href="https://people.eecs.berkeley.edu/~abajcsy/"><strong>Andrea
										Bajcsy</strong>,</br> UC Berkeley</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/Alexandre_Alahi.jpg" alt="" /></span>
							<h5> <a href="https://people.epfl.ch/alexandre.alahi?lang=en"><strong>Alexandre
										Alahi</strong>,</br> EPFL</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/Jiachen_li.jpg" alt="" /></span>
							<h5> <a href="https://jiachenli94.github.io"><strong>Jiachen Li</strong>,</br> Stanford University</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img
									src="images/Organizers/kai-oliver-arras-contact-4x3_res_540x405.jpg"
									alt="" /></span>
							<h5> <a href="https://www.bosch.com/research/know-how/research-experts/dr-kai-oliver-arras/"><strong>Kai
										O. Arras</strong>,</br> Robert Bosch GmbH</a></h5>
						</div>
						<div class="col-2">
							<span class="image fit"><img src="images/Organizers/achim_lilienthal.jpg" alt="" /></span>
							<h5> <a href="http://mro.oru.se/people/achim-lilienthal/"><strong>Achim J.
										Lilienthal</strong>,</br> University of Örebro</a></h5>
						</div>
					</div>
				</div>
				</br>
				
				<header class="major special">
					<h2>Supporting IEEE RAS technical committees</h2>
				</header>
				<div class="row">
					<div class="col-2 col-12-medium">
						<span class="image fit"><img src="images/RAS-logo.png" alt="" /></span>
					</div>
					<div class="col-8 col-12-medium">
						<ul>
							<li><a href="https://www.ieee-ras.org/human-robot-interaction-coordination">Technical
									committee on Human Robot Interaction and Coordination</a></li>
							<li><a href="https://www.ieee-ras.org/algorithms-for-planning-and-control-of-robot-motion">Technical
									committee on Algorithms for Planning and Control of Robot Motion</a></li>
							<li><a href="https://www.ieee-ras.org/human-movement-understanding">Technical committee on
									Human Movement Understanding</a></li>
						</ul>
					</div>
				</div>

				<header class="major special">
					<h2>Supported by</h2>
				</header>
				<div class="box alt">
					<div class="row gtr-uniform">
						<div class="col-2"><span class="image fit"><img style="border-radius:2px" src="images/Affiliations/darko-logo.jpeg"
									alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px" src="images/Affiliations/BoschLogo.jpg"
									alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px"
									src="images/Affiliations/1200px-Örebro_Universitet.svg.png" alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px" src="images/Affiliations/EPFL.svg.png"
									alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px" src="images/Affiliations/uc_berkeley.png"
									alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px"
									src="images/Affiliations/retenua-logo-negative.png" alt="" /></span></div>
						<div class="col-2"><span class="image fit"><img style="border-radius:2px"
									src="images/Affiliations/Stanford.png" alt="" /></span></div>
					</div>
				</div>
			</div>
		</section>

<!--

							<section id="cta" class="main special">
								<header class="major">
									<h2>Congue imperdiet</h2>
									<p>Donec imperdiet consequat consequat. Suspendisse feugiat congue<br />
									posuere. Nulla massa urna, fermentum eget quam aliquet.</p>
								</header>
								<footer class="major">
									<ul class="actions special">
										<li><a href="generic.html" class="button primary">Get Started</a></li>
										<li><a href="generic.html" class="button">Learn More</a></li>
									</ul>
								</footer>
							</section>
-->

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>Get in touch</h2>
							<p>Please feel free to send us an <a href="mailto:Andrey.Rudenko@de.bosch.com?Subject=Workshop%20LHMP"
								target="_top">e-mail</a> , if you have any questions regarding this workshop.</p>
						</section>
						<!-- 
						<section>
							<h2>Etiam feugiat</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>1234 Somewhere Road &bull; Nashville, TN 00000 &bull; USA</dd>
								<dt>Phone</dt>
								<dd>(000) 000-0000 x 0000</dd>
								<dt>Email</dt>
								<dd><a href="#">information@untitled.tld</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="#" class="icon brands fa-twitter alt"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands fa-facebook-f alt"><span class="label">Facebook</span></a></li>
								<li><a href="#" class="icon brands fa-instagram alt"><span class="label">Instagram</span></a></li>
								<li><a href="#" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands fa-dribbble alt"><span class="label">Dribbble</span></a></li>
							</ul>
						</section>
						-->
						<p class="copyright">&copy; Andrey Rudenko and Luigi Palmieri. All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

		<script>
		    const toggleRow = (element) => {
		      element.getElementsByClassName('expanded-row-content')[0].classList.toggle('hide-row');
		      console.log(event);
		    }
		  </script>

	</body>
</html>
